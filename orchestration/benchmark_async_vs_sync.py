#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Benchmark Async vs Sync - Test de performance du pipeline
Compare les performances entre l'architecture async et sync
"""

import asyncio
import time
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List
import json

# Setup paths
sys.path.append(str(Path(__file__).parent.parent))

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PerformanceBenchmark:
    """
    Benchmark pour comparer les performances async vs sync
    """
    
    def __init__(self):
        self.results = {
            "async": {},
            "sync": {},
            "comparison": {}
        }
    
    async def benchmark_async_pipeline(self) -> Dict:
        """
        Test performance du pipeline async
        """
        logger.info("üöÄ Benchmark pipeline ASYNC")
        start_time = time.time()
        
        try:
            from orchestration.async_master_pipeline import AsyncMasterPipeline, PipelineConfig
            
            # Configuration pour le test
            config = PipelineConfig(
                symbols=["BTCUSDT", "ETHUSDT"],
                timeframes=["1h"],
                max_concurrent=4,
                timeout=60
            )
            
            # Cr√©ation du pipeline
            pipeline = AsyncMasterPipeline(config)
            
            # Ex√©cution async
            results = await pipeline.execute_pipeline_async()
            
            execution_time = time.time() - start_time
            
            # Calcul des m√©triques de performance
            async_metrics = {
                "execution_time": execution_time,
                "success_rate": self._calculate_success_rate(results),
                "tasks_completed": len([r for r in results.values() if isinstance(r, dict) and r.get('status') == 'success']),
                "memory_efficiency": "async_optimized",
                "concurrent_tasks": config.max_concurrent,
                "architecture": "async_native"
            }
            
            logger.info(f"‚úÖ Async pipeline termin√© en {execution_time:.2f}s")
            return async_metrics
            
        except Exception as e:
            logger.error(f"‚ùå Erreur benchmark async: {e}")
            return {
                "execution_time": time.time() - start_time,
                "error": str(e),
                "success": False
            }
    
    def benchmark_sync_pipeline(self) -> Dict:
        """
        Test performance du pipeline sync (simulation)
        """
        logger.info("üîÑ Benchmark pipeline SYNC (simulation)")
        start_time = time.time()
        
        try:
            # Simulation du pipeline sync avec les m√™mes t√¢ches
            symbols = ["BTCUSDT", "ETHUSDT"]
            
            # Phase 1: Collecte des donn√©es (s√©quentielle)
            for symbol in symbols:
                time.sleep(0.5)  # Simulation I/O blocking
                logger.debug(f"Sync data collection: {symbol}")
            
            # Phase 2: Feature engineering (s√©quentielle)
            for symbol in symbols:
                time.sleep(1.0)  # Simulation processing
                logger.debug(f"Sync feature engineering: {symbol}")
            
            # Phase 3: Entra√Ænement alphas (s√©quentiel)
            alphas = ["dmn", "mean_reversion", "funding"]
            for alpha in alphas:
                time.sleep(2.0)  # Simulation training
                logger.debug(f"Sync alpha training: {alpha}")
            
            execution_time = time.time() - start_time
            
            sync_metrics = {
                "execution_time": execution_time,
                "success_rate": 1.0,  # Simulation parfaite
                "tasks_completed": len(symbols) * 2 + len(alphas),
                "memory_efficiency": "sync_sequential",
                "concurrent_tasks": 1,
                "architecture": "sync_sequential"
            }
            
            logger.info(f"‚úÖ Sync pipeline simul√© termin√© en {execution_time:.2f}s")
            return sync_metrics
            
        except Exception as e:
            logger.error(f"‚ùå Erreur benchmark sync: {e}")
            return {
                "execution_time": time.time() - start_time,
                "error": str(e),
                "success": False
            }
    
    async def benchmark_async_data_operations(self) -> Dict:
        """
        Benchmark sp√©cifique des op√©rations I/O async
        """
        logger.info("üìä Benchmark I/O operations ASYNC")
        
        try:
            from mlpipeline.utils.async_data import AsyncDataManager
            
            data_manager = AsyncDataManager(max_workers=4)
            
            # Test 1: Chargement parall√®le de fichiers
            test_files = [
                "data/processed/features_BTCUSDT_1h.parquet",
                "data/processed/features_ETHUSDT_1h.parquet"
            ]
            
            # Filtrer les fichiers existants
            existing_files = [f for f in test_files if Path(f).exists()]
            
            if not existing_files:
                logger.warning("‚ö†Ô∏è Pas de fichiers de test trouv√©s")
                return {"status": "no_test_files"}
            
            # Benchmark chargement parall√®le
            start_time = time.time()
            dfs = await data_manager.load_multiple_parquets(existing_files)
            parallel_load_time = time.time() - start_time
            
            # Benchmark chargement s√©quentiel (pour comparaison)
            start_time = time.time()
            sequential_dfs = []
            for file_path in existing_files:
                df = await data_manager.read_parquet_async(file_path)
                sequential_dfs.append(df)
            sequential_load_time = time.time() - start_time
            
            await data_manager.cleanup()
            
            io_metrics = {
                "parallel_load_time": parallel_load_time,
                "sequential_load_time": sequential_load_time,
                "speedup": sequential_load_time / parallel_load_time if parallel_load_time > 0 else 1,
                "files_loaded": len(dfs),
                "total_rows": sum(len(df) for df in dfs)
            }
            
            logger.info(f"‚úÖ I/O async: {io_metrics['speedup']:.2f}x speedup")
            return io_metrics
            
        except Exception as e:
            logger.error(f"‚ùå Erreur benchmark I/O async: {e}")
            return {"error": str(e)}
    
    def _calculate_success_rate(self, results: Dict) -> float:
        """
        Calcule le taux de succ√®s des t√¢ches
        """
        if not results:
            return 0.0
        
        successful = sum(1 for r in results.values() 
                        if isinstance(r, dict) and r.get('status') != 'error')
        total = len(results)
        
        return successful / total if total > 0 else 0.0
    
    async def run_full_benchmark(self) -> Dict:
        """
        Ex√©cute le benchmark complet
        """
        logger.info("üèÅ D√©but benchmark complet Async vs Sync")
        
        # Test 1: Pipeline async
        async_results = await self.benchmark_async_pipeline()
        self.results["async"]["pipeline"] = async_results
        
        # Test 2: Pipeline sync (simulation)
        sync_results = self.benchmark_sync_pipeline()
        self.results["sync"]["pipeline"] = sync_results
        
        # Test 3: Op√©rations I/O async
        io_results = await self.benchmark_async_data_operations()
        self.results["async"]["io_operations"] = io_results
        
        # Calcul de la comparaison
        self._calculate_comparison()
        
        # Sauvegarde des r√©sultats
        await self._save_results()
        
        return self.results
    
    def _calculate_comparison(self):
        """
        Calcule les m√©triques de comparaison
        """
        async_time = self.results["async"]["pipeline"].get("execution_time", 0)
        sync_time = self.results["sync"]["pipeline"].get("execution_time", 0)
        
        if async_time > 0 and sync_time > 0:
            speedup = sync_time / async_time
            efficiency = (sync_time - async_time) / sync_time * 100
        else:
            speedup = 1.0
            efficiency = 0.0
        
        self.results["comparison"] = {
            "speedup": speedup,
            "efficiency_improvement": efficiency,
            "async_time": async_time,
            "sync_time": sync_time,
            "winner": "async" if speedup > 1.0 else "sync",
            "recommendation": self._get_recommendation(speedup, efficiency)
        }
    
    def _get_recommendation(self, speedup: float, efficiency: float) -> str:
        """
        G√©n√®re une recommandation bas√©e sur les r√©sultats
        """
        if speedup > 2.0:
            return "‚úÖ ASYNC HAUTEMENT RECOMMAND√â - Gain de performance significatif"
        elif speedup > 1.5:
            return "‚úÖ ASYNC RECOMMAND√â - Bon gain de performance"
        elif speedup > 1.1:
            return "‚öñÔ∏è ASYNC L√âG√àREMENT AVANTAGEUX - Gain mod√©r√©"
        elif speedup > 0.9:
            return "üü° PERFORMANCES √âQUIVALENTES - Choisir selon architecture"
        else:
            return "‚ö†Ô∏è SYNC PLUS PERFORMANT - Revoir l'impl√©mentation async"
    
    async def _save_results(self):
        """
        Sauvegarde les r√©sultats du benchmark
        """
        try:
            results_dir = Path("benchmark_results")
            results_dir.mkdir(exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            results_file = results_dir / f"benchmark_async_vs_sync_{timestamp}.json"
            
            from mlpipeline.utils.async_data import AsyncDataManager
            data_manager = AsyncDataManager()
            
            await data_manager.write_json_async(self.results, str(results_file))
            await data_manager.cleanup()
            
            logger.info(f"üíæ R√©sultats sauvegard√©s: {results_file}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde: {e}")
    
    def print_summary(self):
        """
        Affiche un r√©sum√© des r√©sultats
        """
        print("\n" + "="*60)
        print("üèÅ R√âSULTATS BENCHMARK ASYNC vs SYNC")
        print("="*60)
        
        comparison = self.results.get("comparison", {})
        async_pipeline = self.results.get("async", {}).get("pipeline", {})
        sync_pipeline = self.results.get("sync", {}).get("pipeline", {})
        io_ops = self.results.get("async", {}).get("io_operations", {})
        
        print(f"‚è±Ô∏è  Temps d'ex√©cution:")
        print(f"   - Async: {async_pipeline.get('execution_time', 0):.2f}s")
        print(f"   - Sync:  {sync_pipeline.get('execution_time', 0):.2f}s")
        
        print(f"\nüìà Performance:")
        print(f"   - Speedup: {comparison.get('speedup', 1):.2f}x")
        print(f"   - Efficacit√©: {comparison.get('efficiency_improvement', 0):.1f}%")
        print(f"   - Gagnant: {comparison.get('winner', 'inconnu').upper()}")
        
        if io_ops.get('speedup'):
            print(f"\nüíæ I/O Operations:")
            print(f"   - Speedup I/O: {io_ops['speedup']:.2f}x")
            print(f"   - Fichiers: {io_ops.get('files_loaded', 0)}")
        
        print(f"\nüéØ Recommandation:")
        print(f"   {comparison.get('recommendation', 'Pas de recommandation')}")
        
        print("="*60)


async def main():
    """
    Point d'entr√©e principal du benchmark
    """
    logger.info("üöÄ D√©marrage benchmark Async vs Sync")
    
    benchmark = PerformanceBenchmark()
    
    try:
        results = await benchmark.run_full_benchmark()
        benchmark.print_summary()
        
        logger.info("‚úÖ Benchmark termin√© avec succ√®s")
        return results
        
    except Exception as e:
        logger.error(f"‚ùå Erreur benchmark: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


if __name__ == "__main__":
    # Ex√©cution avec un seul asyncio.run()
    results = asyncio.run(main())
    
    if results:
        print("\n‚úÖ Benchmark termin√©. V√©rifiez les logs pour les d√©tails.")
    else:
        print("\n‚ùå Benchmark √©chou√©. V√©rifiez les logs pour les erreurs.")