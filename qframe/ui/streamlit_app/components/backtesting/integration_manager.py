"""
Gestionnaire d'int√©gration pour orchestrer tous les composants de backtesting.
Centralise la logique de coordination entre les diff√©rents modules.
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import json
import uuid

from .backtest_configurator import BacktestConfigurator
from .results_analyzer import ResultsAnalyzer
from .walk_forward_interface import WalkForwardInterface
from .monte_carlo_simulator import MonteCarloSimulator
from .performance_analytics import PerformanceAnalytics


class BacktestingIntegrationManager:
    """Gestionnaire d'int√©gration pour l'orchestration des composants de backtesting."""

    def __init__(self):
        """Initialise le gestionnaire d'int√©gration."""
        self.configurator = BacktestConfigurator()
        self.analyzer = ResultsAnalyzer()
        self.walk_forward = WalkForwardInterface()
        self.monte_carlo = MonteCarloSimulator()
        self.analytics = PerformanceAnalytics()

        # Initialisation des √©tats
        self._init_session_state()

    def _init_session_state(self):
        """Initialise les √©tats de session pour le backtesting."""
        if 'bt_manager_initialized' not in st.session_state:
            st.session_state.bt_manager_initialized = True
            st.session_state.backtest_pipeline = {}
            st.session_state.active_backtests = {}
            st.session_state.validation_results = {}

    def create_integrated_workflow(self) -> Dict[str, Any]:
        """Cr√©e un workflow int√©gr√© de backtesting."""
        st.subheader("üîÑ Workflow Int√©gr√© de Backtesting")

        workflow_config = {}

        # √âtape 1: Configuration
        with st.expander("üîß 1. Configuration de Base", expanded=True):
            config_data = self.configurator.render_configuration_section()
            workflow_config['base_config'] = config_data

        # √âtape 2: Validation Walk-Forward
        with st.expander("‚è≠Ô∏è 2. Validation Temporelle", expanded=False):
            if workflow_config.get('base_config'):
                wf_config = self.walk_forward.render_configuration()
                workflow_config['walk_forward'] = wf_config
            else:
                st.info("Compl√©tez d'abord la configuration de base.")

        # √âtape 3: Monte Carlo
        with st.expander("üé≤ 3. Tests de Robustesse", expanded=False):
            if workflow_config.get('base_config'):
                mc_config = self.monte_carlo.render_configuration()
                workflow_config['monte_carlo'] = mc_config
            else:
                st.info("Compl√©tez d'abord la configuration de base.")

        # √âtape 4: Ex√©cution
        with st.expander("‚ñ∂Ô∏è 4. Ex√©cution du Pipeline", expanded=False):
            if self._validate_workflow(workflow_config):
                if st.button("üöÄ Lancer le Pipeline Complet", type="primary"):
                    self._execute_integrated_pipeline(workflow_config)
            else:
                st.warning("Veuillez compl√©ter toutes les configurations requises.")

        return workflow_config

    def render_pipeline_dashboard(self):
        """Affiche le tableau de bord du pipeline."""
        st.subheader("üìä Dashboard du Pipeline")

        if not st.session_state.get('active_backtests'):
            st.info("Aucun backtest en cours. Lancez un pipeline pour commencer.")
            return

        # Status des backtests actifs
        col1, col2, col3 = st.columns(3)

        with col1:
            active_count = len([bt for bt in st.session_state.active_backtests.values()
                              if bt['status'] == 'running'])
            st.metric("Backtests Actifs", active_count)

        with col2:
            completed_count = len([bt for bt in st.session_state.active_backtests.values()
                                 if bt['status'] == 'completed'])
            st.metric("Backtests Termin√©s", completed_count)

        with col3:
            failed_count = len([bt for bt in st.session_state.active_backtests.values()
                              if bt['status'] == 'failed'])
            st.metric("Backtests √âchou√©s", failed_count)

        # Table des backtests
        self._render_backtests_table()

    def generate_integrated_report(self, backtest_id: str) -> Dict[str, Any]:
        """G√©n√®re un rapport int√©gr√© complet."""
        if backtest_id not in st.session_state.get('backtest_results', {}):
            st.error("R√©sultats de backtest non trouv√©s.")
            return {}

        results = st.session_state.backtest_results[backtest_id]

        st.subheader("üìã Rapport Int√©gr√© Complet")

        # Onglets du rapport
        report_tabs = st.tabs([
            "üìà Performance",
            "üîç Analytics",
            "‚è≠Ô∏è Walk-Forward",
            "üé≤ Monte Carlo",
            "üìä Comparaisons"
        ])

        with report_tabs[0]:
            self.analyzer.render_performance_summary(results)

        with report_tabs[1]:
            self.analytics.render_advanced_analytics(results)

        with report_tabs[2]:
            if 'walk_forward_results' in results:
                self.walk_forward.render_results(results['walk_forward_results'])
            else:
                st.info("Pas de r√©sultats Walk-Forward disponibles.")

        with report_tabs[3]:
            if 'monte_carlo_results' in results:
                self.monte_carlo.render_results(results['monte_carlo_results'])
            else:
                st.info("Pas de r√©sultats Monte Carlo disponibles.")

        with report_tabs[4]:
            self._render_strategy_comparison()

        return self._compile_report_data(results)

    def export_comprehensive_results(self, backtest_id: str) -> Dict[str, Any]:
        """Exporte les r√©sultats complets dans diff√©rents formats."""
        st.subheader("üíæ Export des R√©sultats")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("üìä Export Excel"):
                excel_data = self._generate_excel_export(backtest_id)
                st.success("Export Excel g√©n√©r√©!")
                return excel_data

        with col2:
            if st.button("üìÑ Export PDF"):
                pdf_data = self._generate_pdf_report(backtest_id)
                st.success("Rapport PDF g√©n√©r√©!")
                return pdf_data

        with col3:
            if st.button("üìã Export JSON"):
                json_data = self._generate_json_export(backtest_id)
                st.success("Export JSON g√©n√©r√©!")
                return json_data

    def _validate_workflow(self, workflow_config: Dict) -> bool:
        """Valide la configuration du workflow."""
        required_keys = ['base_config']
        return all(key in workflow_config and workflow_config[key] for key in required_keys)

    def _execute_integrated_pipeline(self, workflow_config: Dict):
        """Ex√©cute le pipeline int√©gr√© de backtesting."""
        pipeline_id = str(uuid.uuid4())[:8]

        # Initialisation du pipeline
        pipeline_data = {
            'id': pipeline_id,
            'config': workflow_config,
            'status': 'running',
            'started_at': datetime.now(),
            'steps_completed': 0,
            'total_steps': 4,
            'results': {}
        }

        st.session_state.active_backtests[pipeline_id] = pipeline_data

        progress_bar = st.progress(0)
        status_text = st.empty()

        try:
            # √âtape 1: Backtest de base
            status_text.text("üîÑ Ex√©cution du backtest de base...")
            base_results = self._run_base_backtest(workflow_config['base_config'])
            pipeline_data['results']['base'] = base_results
            pipeline_data['steps_completed'] = 1
            progress_bar.progress(0.25)

            # √âtape 2: Walk-Forward (si configur√©)
            if 'walk_forward' in workflow_config:
                status_text.text("‚è≠Ô∏è Validation Walk-Forward...")
                wf_results = self._run_walk_forward_analysis(workflow_config['walk_forward'])
                pipeline_data['results']['walk_forward'] = wf_results
                pipeline_data['steps_completed'] = 2
                progress_bar.progress(0.5)

            # √âtape 3: Monte Carlo (si configur√©)
            if 'monte_carlo' in workflow_config:
                status_text.text("üé≤ Simulation Monte Carlo...")
                mc_results = self._run_monte_carlo_simulation(workflow_config['monte_carlo'])
                pipeline_data['results']['monte_carlo'] = mc_results
                pipeline_data['steps_completed'] = 3
                progress_bar.progress(0.75)

            # √âtape 4: Analyse int√©gr√©e
            status_text.text("üìä G√©n√©ration de l'analyse int√©gr√©e...")
            integrated_results = self._compile_integrated_results(pipeline_data['results'])
            pipeline_data['results']['integrated'] = integrated_results
            pipeline_data['steps_completed'] = 4
            progress_bar.progress(1.0)

            # Finalisation
            pipeline_data['status'] = 'completed'
            pipeline_data['completed_at'] = datetime.now()

            # Sauvegarde des r√©sultats
            if pipeline_id not in st.session_state.backtest_results:
                st.session_state.backtest_results = {}
            st.session_state.backtest_results[pipeline_id] = integrated_results

            status_text.text("‚úÖ Pipeline termin√© avec succ√®s!")
            st.success(f"Pipeline {pipeline_id} termin√© avec succ√®s!")

        except Exception as e:
            pipeline_data['status'] = 'failed'
            pipeline_data['error'] = str(e)
            status_text.text(f"‚ùå Erreur: {str(e)}")
            st.error(f"Erreur lors de l'ex√©cution du pipeline: {str(e)}")

    def _render_backtests_table(self):
        """Affiche le tableau des backtests."""
        if not st.session_state.get('active_backtests'):
            return

        backtests_data = []
        for bt_id, bt_data in st.session_state.active_backtests.items():
            backtests_data.append({
                'ID': bt_id,
                'Status': bt_data['status'],
                'D√©marr√©': bt_data['started_at'].strftime('%H:%M:%S'),
                '√âtapes': f"{bt_data['steps_completed']}/{bt_data['total_steps']}",
                'Dur√©e': self._calculate_duration(bt_data['started_at'])
            })

        if backtests_data:
            df = pd.DataFrame(backtests_data)
            st.dataframe(df, use_container_width=True)

    def _run_base_backtest(self, config: Dict) -> Dict:
        """Ex√©cute un backtest de base."""
        # Simulation d'un backtest avec r√©sultats r√©alistes
        return self._generate_simulated_backtest_results(config)

    def _run_walk_forward_analysis(self, config: Dict) -> Dict:
        """Ex√©cute une analyse Walk-Forward."""
        return self.walk_forward._generate_simulated_results(config)

    def _run_monte_carlo_simulation(self, config: Dict) -> Dict:
        """Ex√©cute une simulation Monte Carlo."""
        return self.monte_carlo._generate_mc_results(config)

    def _compile_integrated_results(self, results: Dict) -> Dict:
        """Compile les r√©sultats de tous les composants."""
        integrated = results.get('base', {})

        # Int√©gration des r√©sultats Walk-Forward
        if 'walk_forward' in results:
            integrated['walk_forward_results'] = results['walk_forward']

        # Int√©gration des r√©sultats Monte Carlo
        if 'monte_carlo' in results:
            integrated['monte_carlo_results'] = results['monte_carlo']

        # M√©triques consolid√©es
        integrated['integration_score'] = self._calculate_integration_score(results)
        integrated['confidence_level'] = self._calculate_confidence_level(results)

        return integrated

    def _generate_simulated_backtest_results(self, config: Dict) -> Dict:
        """G√©n√®re des r√©sultats de backtest simul√©s."""
        np.random.seed(42)  # Pour la reproductibilit√©

        # G√©n√©ration de la s√©rie de prix
        days = 252
        initial_price = 10000
        dates = pd.date_range(start='2024-01-01', periods=days, freq='D')

        # Simulation d'une strat√©gie avec drift positif
        daily_returns = np.random.normal(0.0008, 0.02, days)  # ~20% annuel avec 20% vol
        daily_returns[0] = 0  # Premier jour = 0

        cumulative_returns = np.cumprod(1 + daily_returns)
        equity_curve = initial_price * cumulative_returns

        # Calcul des m√©triques
        total_return = (equity_curve[-1] / equity_curve[0] - 1) * 100
        annualized_return = ((equity_curve[-1] / equity_curve[0]) ** (252/days) - 1) * 100
        volatility = np.std(daily_returns) * np.sqrt(252) * 100

        # Sharpe ratio
        risk_free_rate = 0.02
        excess_returns = daily_returns - risk_free_rate/252
        sharpe_ratio = np.mean(excess_returns) / np.std(daily_returns) * np.sqrt(252)

        # Drawdown
        rolling_max = np.maximum.accumulate(equity_curve)
        drawdown = (equity_curve - rolling_max) / rolling_max * 100
        max_drawdown = np.min(drawdown)

        # M√©triques de trading
        num_trades = np.random.randint(80, 150)
        win_rate = np.random.uniform(45, 65)
        profit_factor = np.random.uniform(1.1, 2.5)

        return {
            'total_return': total_return,
            'annualized_return': annualized_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'sortino_ratio': sharpe_ratio * 1.2,  # Approximation
            'calmar_ratio': annualized_return / abs(max_drawdown) if max_drawdown != 0 else 0,
            'var_95': np.percentile(daily_returns, 5) * 100,
            'cvar_95': np.mean(daily_returns[daily_returns <= np.percentile(daily_returns, 5)]) * 100,
            'win_rate': win_rate,
            'total_trades': num_trades,
            'profit_factor': profit_factor,
            'avg_trade': total_return / num_trades,
            'avg_win': total_return / (num_trades * win_rate / 100),
            'avg_loss': -total_return / (num_trades * (100 - win_rate) / 100),
            'best_month': np.max(daily_returns) * 100,
            'worst_month': np.min(daily_returns) * 100,
            'equity_curve': {
                'dates': dates.tolist(),
                'values': equity_curve.tolist()
            },
            'drawdown_series': drawdown.tolist(),
            'monthly_returns': np.random.normal(1.5, 4.0, 12).tolist(),
            'benchmark_curve': {
                'dates': dates.tolist(),
                'values': (initial_price * np.cumprod(1 + np.random.normal(0.0005, 0.015, days))).tolist()
            }
        }

    def _calculate_integration_score(self, results: Dict) -> float:
        """Calcule un score d'int√©gration bas√© sur la coh√©rence des r√©sultats."""
        base_score = 0.7  # Score de base

        # Bonus pour la coh√©rence Walk-Forward
        if 'walk_forward' in results:
            base_score += 0.15

        # Bonus pour la robustesse Monte Carlo
        if 'monte_carlo' in results:
            base_score += 0.15

        return min(base_score, 1.0)

    def _calculate_confidence_level(self, results: Dict) -> float:
        """Calcule le niveau de confiance global."""
        confidence = 0.6  # Base

        # Augmentation bas√©e sur les tests
        if 'walk_forward' in results:
            confidence += 0.2
        if 'monte_carlo' in results:
            confidence += 0.2

        return min(confidence, 0.95)

    def _render_strategy_comparison(self):
        """Affiche la comparaison des strat√©gies."""
        if len(st.session_state.get('backtest_results', {})) < 2:
            st.info("Besoin d'au moins 2 r√©sultats pour la comparaison.")
            return

        # Table de comparaison simple
        comparison_data = []
        for bt_id, results in st.session_state.backtest_results.items():
            comparison_data.append({
                'Strategy': bt_id,
                'Total Return (%)': f"{results.get('total_return', 0):.1f}",
                'Sharpe Ratio': f"{results.get('sharpe_ratio', 0):.2f}",
                'Max DD (%)': f"{results.get('max_drawdown', 0):.1f}",
                'Win Rate (%)': f"{results.get('win_rate', 0):.1f}"
            })

        if comparison_data:
            df = pd.DataFrame(comparison_data)
            st.dataframe(df, use_container_width=True)

    def _calculate_duration(self, start_time: datetime) -> str:
        """Calcule la dur√©e √©coul√©e."""
        duration = datetime.now() - start_time
        seconds = int(duration.total_seconds())
        if seconds < 60:
            return f"{seconds}s"
        elif seconds < 3600:
            return f"{seconds//60}m {seconds%60}s"
        else:
            hours = seconds // 3600
            minutes = (seconds % 3600) // 60
            return f"{hours}h {minutes}m"

    def _compile_report_data(self, results: Dict) -> Dict:
        """Compile les donn√©es pour le rapport."""
        return {
            'executive_summary': self._generate_executive_summary(results),
            'detailed_metrics': results,
            'recommendations': self._generate_recommendations(results),
            'risk_assessment': self._generate_risk_assessment(results)
        }

    def _generate_executive_summary(self, results: Dict) -> str:
        """G√©n√®re un r√©sum√© ex√©cutif."""
        total_return = results.get('total_return', 0)
        sharpe = results.get('sharpe_ratio', 0)
        max_dd = results.get('max_drawdown', 0)

        performance_rating = "Excellente" if total_return > 15 else "Bonne" if total_return > 5 else "Mod√©r√©e"

        return f"""
        La strat√©gie a g√©n√©r√© un retour total de {total_return:.1f}% avec un ratio de Sharpe de {sharpe:.2f}.
        Le drawdown maximal de {abs(max_dd):.1f}% indique un niveau de risque contr√¥l√©.
        Performance globale: {performance_rating}.
        """

    def _generate_recommendations(self, results: Dict) -> List[str]:
        """G√©n√®re des recommandations."""
        recommendations = []

        sharpe = results.get('sharpe_ratio', 0)
        if sharpe < 1.0:
            recommendations.append("Consid√©rer l'optimisation des param√®tres pour am√©liorer le Sharpe ratio")

        max_dd = results.get('max_drawdown', 0)
        if abs(max_dd) > 20:
            recommendations.append("Implementer des m√©canismes de stop-loss plus stricts")

        win_rate = results.get('win_rate', 0)
        if win_rate < 50:
            recommendations.append("Analyser les signaux pour am√©liorer la pr√©cision")

        return recommendations

    def _generate_risk_assessment(self, results: Dict) -> Dict:
        """G√©n√®re une √©valuation des risques."""
        var_95 = results.get('var_95', 0)
        max_dd = results.get('max_drawdown', 0)

        risk_level = "√âlev√©" if abs(max_dd) > 25 else "Mod√©r√©" if abs(max_dd) > 15 else "Faible"

        return {
            'risk_level': risk_level,
            'var_assessment': f"VaR 95%: {var_95:.2f}%",
            'drawdown_assessment': f"Drawdown maximal: {abs(max_dd):.1f}%",
            'overall_risk': risk_level
        }

    def _generate_excel_export(self, backtest_id: str) -> Dict:
        """G√©n√®re un export Excel."""
        return {'format': 'excel', 'data': 'Excel export data'}

    def _generate_pdf_report(self, backtest_id: str) -> Dict:
        """G√©n√®re un rapport PDF."""
        return {'format': 'pdf', 'data': 'PDF report data'}

    def _generate_json_export(self, backtest_id: str) -> Dict:
        """G√©n√®re un export JSON."""
        return {'format': 'json', 'data': st.session_state.backtest_results.get(backtest_id, {})}