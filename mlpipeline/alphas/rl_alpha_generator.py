"""
GÃ©nÃ©rateur RL d'Alphas Automatique
BasÃ© sur "Synergistic Formulaic Alpha Generation for Quantitative Trading"

ImplÃ©mente un agent PPO qui gÃ©nÃ¨re automatiquement des formules alpha
en combinant intelligemment les opÃ©rateurs symboliques.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from typing import List, Dict, Tuple, Optional
import random
import logging
from dataclasses import dataclass
from abc import ABC, abstractmethod

from mlpipeline.features.symbolic_operators import SymbolicOperators

logger = logging.getLogger(__name__)

@dataclass
class AlphaFormula:
    """ReprÃ©sente une formule alpha gÃ©nÃ©rÃ©e"""
    formula: str
    operators: List[str]
    ic: float
    rank_ic: float
    complexity: int
    generation: int

class SearchSpace:
    """DÃ©finit l'espace de recherche pour la gÃ©nÃ©ration d'alphas"""

    def __init__(self):
        # OpÃ©rateurs disponibles (du papier)
        self.operators = [
            'sign', 'cs_rank', 'product', 'scale', 'pow_op',
            'skew', 'kurt', 'ts_rank', 'delta', 'argmax', 'argmin',
            'cond', 'wma', 'ema', 'mad'
        ]

        # Features de base
        self.features = ['open', 'high', 'low', 'close', 'volume', 'vwap']

        # Constantes et paramÃ¨tres temporels
        self.constants = [-2.0, -1.0, -0.5, 0.5, 1.0, 2.0, 5.0, 10.0]
        self.time_deltas = [5, 10, 20, 30, 40, 50, 60, 120]

        # Taille maximale des formules
        self.max_depth = 4
        self.max_complexity = 10

    def get_action_space_size(self) -> int:
        """Retourne la taille de l'espace d'action"""
        return len(self.operators) + len(self.features) + len(self.constants) + len(self.time_deltas)

class FormulaEnvironment:
    """Environnement RL pour la gÃ©nÃ©ration de formules alpha"""

    def __init__(self, market_data: pd.DataFrame, search_space: SearchSpace):
        self.data = market_data
        self.search_space = search_space
        self.ops = SymbolicOperators()
        self.current_formula = []
        self.current_complexity = 0

    def reset(self) -> np.ndarray:
        """Reset l'environnement pour un nouvel Ã©pisode"""
        self.current_formula = []
        self.current_complexity = 0
        return self._get_state()

    def _get_state(self) -> np.ndarray:
        """Retourne l'Ã©tat actuel de l'environnement"""
        # Ã‰tat basÃ© sur la formule courante et statistiques des donnÃ©es
        state = np.zeros(50)  # Ã‰tat de taille fixe

        # Encoder la formule courante
        for i, element in enumerate(self.current_formula[-10:]):  # Derniers 10 Ã©lÃ©ments
            if i < 10:
                state[i] = hash(str(element)) % 1000 / 1000.0

        # Statistiques des donnÃ©es
        if len(self.data) > 0:
            state[10:15] = [
                self.data['close'].mean() / 50000,  # Prix normalisÃ©
                self.data['volume'].mean() / 10000,  # Volume normalisÃ©
                self.data['close'].std() / 1000,     # VolatilitÃ©
                len(self.data) / 1000,               # Taille dataset
                self.current_complexity / 10         # ComplexitÃ© courante
            ]

        return state.astype(np.float32)

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """ExÃ©cute une action dans l'environnement"""
        # DÃ©coder l'action
        element = self._decode_action(action)

        # Ajouter l'Ã©lÃ©ment Ã  la formule
        self.current_formula.append(element)
        self.current_complexity += 1

        # Calculer la rÃ©compense
        reward = self._calculate_reward()

        # VÃ©rifier si l'Ã©pisode est terminÃ©
        done = (self.current_complexity >= self.search_space.max_complexity or
                len(self.current_formula) >= 20)

        return self._get_state(), reward, done, {}

    def _decode_action(self, action: int) -> str:
        """DÃ©code une action en Ã©lÃ©ment de formule"""
        total_ops = len(self.search_space.operators)
        total_features = len(self.search_space.features)
        total_constants = len(self.search_space.constants)

        if action < total_ops:
            return self.search_space.operators[action]
        elif action < total_ops + total_features:
            return self.search_space.features[action - total_ops]
        elif action < total_ops + total_features + total_constants:
            return str(self.search_space.constants[action - total_ops - total_features])
        else:
            return str(self.search_space.time_deltas[action - total_ops - total_features - total_constants])

    def _calculate_reward(self) -> float:
        """Calcule la rÃ©compense pour la formule courante"""
        if len(self.current_formula) < 3:  # Formule trop courte
            return -0.1

        try:
            # Essayer d'Ã©valuer la formule
            alpha_values = self._evaluate_formula()

            if alpha_values is None or len(alpha_values) == 0:
                return -1.0  # Formule invalide

            # Calculer IC (Information Coefficient)
            ic = self._calculate_ic(alpha_values)

            # RÃ©compense basÃ©e sur IC avec pÃ©nalitÃ© pour complexitÃ©
            complexity_penalty = self.current_complexity * 0.01
            return ic - complexity_penalty

        except Exception:
            return -1.0  # Erreur d'Ã©valuation

    def _evaluate_formula(self) -> Optional[pd.Series]:
        """Ã‰value la formule courante"""
        try:
            # Simplification : crÃ©er une formule basique combinant les Ã©lÃ©ments
            if len(self.current_formula) < 3:
                return None

            # Prendre les 3 derniers Ã©lÃ©ments significatifs
            relevant_elements = [e for e in self.current_formula if e in self.search_space.operators + self.search_space.features][-3:]

            if len(relevant_elements) < 2:
                return None

            # Construire une formule simple
            if 'close' in self.data.columns and 'volume' in self.data.columns:
                if 'cs_rank' in relevant_elements:
                    return self.ops.cs_rank(self.data['volume'])
                elif 'sign' in relevant_elements:
                    returns = self.data['close'].pct_change().fillna(0)
                    return self.ops.sign(returns)
                elif 'delta' in relevant_elements:
                    return self.ops.delta(self.data['close'], 5)
                else:
                    # Formule par dÃ©faut
                    return self.data['close'].rolling(10).corr(self.data['volume'])

            return None

        except Exception as e:
            logger.debug(f"Erreur Ã©valuation formule: {e}")
            return None

    def _calculate_ic(self, alpha_values: pd.Series) -> float:
        """Calcule l'Information Coefficient"""
        try:
            if 'ret' in self.data.columns:
                # Utiliser les returns futurs
                future_returns = self.data['ret'].shift(-1).fillna(0)
                aligned_alpha = alpha_values.reindex(future_returns.index).fillna(0)

                # CorrÃ©lation de Pearson
                correlation = aligned_alpha.corr(future_returns)
                return correlation if not np.isnan(correlation) else 0.0
            else:
                # CorrÃ©lation avec close price changes
                price_changes = self.data['close'].pct_change().fillna(0)
                aligned_alpha = alpha_values.reindex(price_changes.index).fillna(0)

                correlation = aligned_alpha.corr(price_changes)
                return correlation if not np.isnan(correlation) else 0.0

        except Exception:
            return 0.0

class PPOAgent(nn.Module):
    """Agent PPO pour la gÃ©nÃ©ration d'alphas"""

    def __init__(self, state_size: int, action_size: int, hidden_size: int = 128):
        super().__init__()

        # RÃ©seau de politique (actor)
        self.actor = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, action_size),
            nn.Softmax(dim=-1)
        )

        # RÃ©seau de valeur (critic)
        self.critic = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, 1)
        )

    def forward(self, state):
        action_probs = self.actor(state)
        value = self.critic(state)
        return action_probs, value

    def get_action(self, state):
        """SÃ©lectionne une action basÃ©e sur la politique"""
        action_probs, _ = self.forward(state)
        dist = torch.distributions.Categorical(action_probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)

class RLAlphaGenerator:
    """GÃ©nÃ©rateur principal d'alphas utilisant RL"""

    def __init__(self, market_data: pd.DataFrame):
        self.data = market_data
        self.search_space = SearchSpace()
        self.env = FormulaEnvironment(market_data, self.search_space)

        # ParamÃ¨tres PPO
        self.state_size = 50
        self.action_size = self.search_space.get_action_space_size()

        # Agent PPO
        self.agent = PPOAgent(self.state_size, self.action_size)
        self.optimizer = optim.Adam(self.agent.parameters(), lr=3e-4)

        # Historique des alphas gÃ©nÃ©rÃ©s
        self.generated_alphas: List[AlphaFormula] = []
        self.generation_count = 0

        logger.info(f"âœ… RL Alpha Generator initialisÃ© - Action space: {self.action_size}")

    def generate_alpha_batch(self, num_episodes: int = 10) -> List[AlphaFormula]:
        """GÃ©nÃ¨re un batch d'alphas"""
        new_alphas = []

        for episode in range(num_episodes):
            try:
                alpha = self._generate_single_alpha()
                if alpha and alpha.ic > 0.01:  # Seuil minimum d'IC
                    new_alphas.append(alpha)
                    logger.info(f"ğŸ“Š Alpha gÃ©nÃ©rÃ© - IC: {alpha.ic:.4f}, ComplexitÃ©: {alpha.complexity}")
            except Exception as e:
                logger.debug(f"Erreur gÃ©nÃ©ration Ã©pisode {episode}: {e}")

        self.generated_alphas.extend(new_alphas)
        self.generation_count += 1

        logger.info(f"ğŸ¯ Batch terminÃ©: {len(new_alphas)}/{num_episodes} alphas valides")
        return new_alphas

    def _generate_single_alpha(self) -> Optional[AlphaFormula]:
        """GÃ©nÃ¨re un seul alpha"""
        state = torch.FloatTensor(self.env.reset()).unsqueeze(0)
        done = False
        total_reward = 0
        actions = []

        while not done:
            action, log_prob = self.agent.get_action(state)
            next_state, reward, done, _ = self.env.step(action)

            actions.append(action)
            total_reward += reward
            state = torch.FloatTensor(next_state).unsqueeze(0)

        # CrÃ©er l'objet AlphaFormula
        if total_reward > -0.5:  # Seuil de qualitÃ© minimum
            formula_str = self._actions_to_formula_string(actions)

            return AlphaFormula(
                formula=formula_str,
                operators=self.env.current_formula,
                ic=max(0, total_reward),
                rank_ic=max(0, total_reward * 0.8),  # Approximation
                complexity=self.env.current_complexity,
                generation=self.generation_count
            )

        return None

    def _actions_to_formula_string(self, actions: List[int]) -> str:
        """Convertit une sÃ©quence d'actions en string de formule"""
        elements = [self.env._decode_action(a) for a in actions]

        # Simplification : crÃ©er une reprÃ©sentation readable
        ops = [e for e in elements if e in self.search_space.operators]
        features = [e for e in elements if e in self.search_space.features]

        if ops and features:
            return f"{ops[0]}({features[0] if features else 'close'})"
        else:
            return "simple_formula"

    def get_best_alphas(self, top_k: int = 5) -> List[AlphaFormula]:
        """Retourne les meilleurs alphas gÃ©nÃ©rÃ©s"""
        if not self.generated_alphas:
            return []

        sorted_alphas = sorted(self.generated_alphas, key=lambda x: x.ic, reverse=True)
        return sorted_alphas[:top_k]

    def get_generation_stats(self) -> Dict:
        """Retourne les statistiques de gÃ©nÃ©ration"""
        if not self.generated_alphas:
            return {"total": 0, "mean_ic": 0, "best_ic": 0}

        ics = [alpha.ic for alpha in self.generated_alphas]

        return {
            "total_generated": len(self.generated_alphas),
            "generations": self.generation_count,
            "mean_ic": np.mean(ics),
            "best_ic": max(ics),
            "std_ic": np.std(ics),
            "valid_alphas": len([a for a in self.generated_alphas if a.ic > 0.01])
        }

def test_rl_generator():
    """Test du gÃ©nÃ©rateur RL"""
    print("ğŸ§ª Test du gÃ©nÃ©rateur RL d'alphas")

    # CrÃ©er des donnÃ©es de test
    np.random.seed(42)
    dates = pd.date_range('2024-01-01', periods=200, freq='1h')

    data = pd.DataFrame({
        'timestamp': dates,
        'open': 50000 + np.cumsum(np.random.randn(200) * 100),
        'high': 50100 + np.cumsum(np.random.randn(200) * 100),
        'low': 49900 + np.cumsum(np.random.randn(200) * 100),
        'close': 50000 + np.cumsum(np.random.randn(200) * 100),
        'volume': np.random.randint(1000, 10000, 200),
        'ret': np.random.randn(200) * 0.02
    }, index=dates)

    data['vwap'] = (data['high'] + data['low'] + data['close']) / 3

    # CrÃ©er le gÃ©nÃ©rateur
    generator = RLAlphaGenerator(data)

    # GÃ©nÃ©rer des alphas
    print("ğŸ¯ GÃ©nÃ©ration d'alphas...")
    alphas = generator.generate_alpha_batch(5)

    print(f"âœ… Alphas gÃ©nÃ©rÃ©s: {len(alphas)}")

    # Statistiques
    stats = generator.get_generation_stats()
    print(f"ğŸ“Š Stats: {stats}")

    # Meilleurs alphas
    best = generator.get_best_alphas(3)
    for i, alpha in enumerate(best):
        print(f"ğŸ† Top {i+1}: IC={alpha.ic:.4f}, Formule={alpha.formula}")

    return generator

if __name__ == "__main__":
    test_rl_generator()