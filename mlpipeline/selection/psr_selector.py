#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Probabilistic Sharpe Ratio (PSR) Selector - Version Production
Impl√©mentation compl√®te selon Marcos L√≥pez de Prado
Migration et am√©lioration du selector_promote.py original
- PSR selon formulation acad√©mique rigoureuse
- Multi-crit√®res avec pond√©ration adaptative
- Analyse de stabilit√© temporelle
- Tests statistiques avanc√©s
UNIQUEMENT donn√©es r√©elles - Validation stricte
"""

import logging
import os
import sys
import json
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import glob

import numpy as np
import pandas as pd
from scipy import stats
from scipy.optimize import minimize
from sklearn.preprocessing import StandardScaler
import mlflow
import mlflow.sklearn

# Configuration path pour imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# Import des utilitaires
try:
    from mlpipeline.utils.risk_metrics import (
        ratio_sharpe, drawdown_max, probabilistic_sharpe_ratio,
        comprehensive_metrics, validate_metrics
    )
    from mlpipeline.utils.artifact_cleaner import validate_real_data_only
except ImportError:
    # Fallback pour ex√©cution comme script
    logger.warning("Utilitaires non disponibles - fonctions simul√©es")
    
    def ratio_sharpe(returns):
        return returns.mean() / returns.std() * np.sqrt(252) if returns.std() > 0 else 0
    
    def drawdown_max(equity_curve):
        peak = equity_curve.cummax()
        drawdown = (equity_curve - peak) / peak
        return abs(drawdown.min())
    
    def probabilistic_sharpe_ratio(returns, benchmark=0):
        n = len(returns)
        sharpe = ratio_sharpe(returns)
        return 1 - stats.norm.cdf((benchmark - sharpe) * np.sqrt(n-1))
    
    def comprehensive_metrics(returns):
        return {"sharpe": ratio_sharpe(returns), "max_drawdown": drawdown_max((1+returns).cumprod())}
    
    def validate_metrics(metrics):
        return True
    
    def validate_real_data_only(df, source="API"):
        return not df.empty

logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore', category=UserWarning)

class PSRSelector:
    """
    S√©lecteur de strat√©gies bas√© sur le Probabilistic Sharpe Ratio
    
    Impl√©mentation selon L√≥pez de Prado "Advances in Financial Machine Learning"
    Am√©liorations vs original :
    - PSR exact avec correction de biais
    - Tests de stabilit√© temporelle
    - S√©lection multi-crit√®res optimis√©e  
    - Analyse de corr√©lation entre alphas
    - Bootstrap pour validation robustesse
    """
    
    def __init__(self,
                 psr_threshold: float = 0.65,
                 sharpe_threshold: float = 1.0,
                 maxdd_threshold: float = 0.25,
                 min_observations: int = 252,
                 confidence_level: float = 0.95,
                 stability_window: int = 63):  # 1 trimestre
        
        self.psr_threshold = psr_threshold
        self.sharpe_threshold = sharpe_threshold
        self.maxdd_threshold = maxdd_threshold
        self.min_observations = min_observations
        self.confidence_level = confidence_level
        self.stability_window = stability_window
        
        # Cache des m√©triques calcul√©es
        self.metrics_cache = {}
        self.stability_analysis = {}
        
        logger.info(f"‚úÖ PSR Selector initialis√© (seuil PSR={psr_threshold:.2f})")
    
    def calculate_precise_psr(self, returns: np.ndarray, 
                             benchmark_sharpe: float = 0.0,
                             frequency: int = 365) -> Dict[str, float]:
        """
        Calcul pr√©cis du PSR selon L√≥pez de Prado
        
        PSR = Œ¶((SRÃÇ - SR*) / œÉÃÇ(SRÃÇ))
        
        o√π:
        - SRÃÇ = Sharpe ratio observ√©
        - SR* = Sharpe ratio de r√©f√©rence
        - œÉÃÇ(SRÃÇ) = √âcart-type estim√© du Sharpe ratio
        - Œ¶ = CDF de la loi normale
        """
        
        returns = np.asarray(returns)
        n_obs = len(returns)
        
        if n_obs < self.min_observations:
            return {
                "psr": 0.0,
                "sharpe_observed": 0.0,
                "sharpe_stderr": np.inf,
                "psr_confidence_interval": (0.0, 0.0)
            }
        
        # Sharpe ratio observ√©
        mean_ret = np.mean(returns)
        std_ret = np.std(returns, ddof=1)
        
        if std_ret <= 0:
            return {
                "psr": 0.0,
                "sharpe_observed": 0.0,
                "sharpe_stderr": np.inf,
                "psr_confidence_interval": (0.0, 0.0)
            }
        
        sharpe_observed = (mean_ret / std_ret) * np.sqrt(frequency)
        
        # Moments d'ordre sup√©rieur pour correction de biais
        skewness = stats.skew(returns)
        excess_kurtosis = stats.kurtosis(returns, fisher=True)
        
        # Variance du Sharpe ratio (formule de L√≥pez de Prado)
        sharpe_variance = (1 / n_obs) * (
            1 + 0.5 * sharpe_observed**2 - 
            skewness * sharpe_observed + 
            (excess_kurtosis / 4) * sharpe_observed**2
        )
        
        if sharpe_variance <= 0:
            return {
                "psr": 0.0,
                "sharpe_observed": sharpe_observed,
                "sharpe_stderr": np.inf,
                "psr_confidence_interval": (0.0, 0.0)
            }
        
        sharpe_stderr = np.sqrt(sharpe_variance)
        
        # Test statistique
        z_score = (sharpe_observed - benchmark_sharpe) / sharpe_stderr
        
        # PSR = P(Sharpe > benchmark)
        psr = stats.norm.cdf(z_score)
        
        # Intervalle de confiance du PSR
        z_alpha = stats.norm.ppf((1 + self.confidence_level) / 2)
        psr_lower = stats.norm.cdf(z_score - z_alpha)
        psr_upper = stats.norm.cdf(z_score + z_alpha)
        
        return {
            "psr": float(psr),
            "sharpe_observed": float(sharpe_observed),
            "sharpe_stderr": float(sharpe_stderr),
            "z_score": float(z_score),
            "psr_confidence_interval": (float(psr_lower), float(psr_upper)),
            "skewness": float(skewness),
            "excess_kurtosis": float(excess_kurtosis),
            "n_observations": int(n_obs)
        }
    
    def analyze_temporal_stability(self, returns: np.ndarray) -> Dict[str, float]:
        """
        Analyse de stabilit√© temporelle des performances
        
        Divise la s√©rie en sous-p√©riodes et analyse la consistance
        du Sharpe ratio
        """
        
        returns = np.asarray(returns)
        n_obs = len(returns)
        
        if n_obs < self.stability_window * 3:
            return {
                "stability_score": 0.0,
                "sharpe_consistency": 0.0,
                "periods_positive": 0.0
            }
        
        # Division en p√©riodes de taille stability_window
        n_periods = n_obs // self.stability_window
        period_sharpes = []
        period_returns = []
        
        for i in range(n_periods):
            start_idx = i * self.stability_window
            end_idx = (i + 1) * self.stability_window
            period_rets = returns[start_idx:end_idx]
            
            if len(period_rets) > 0 and np.std(period_rets) > 0:
                period_sharpe = (np.mean(period_rets) / np.std(period_rets)) * np.sqrt(365)
                period_sharpes.append(period_sharpe)
                period_returns.append(np.mean(period_rets) * 365)  # Annualis√©
        
        if len(period_sharpes) < 2:
            return {
                "stability_score": 0.0,
                "sharpe_consistency": 0.0,
                "periods_positive": 0.0
            }
        
        period_sharpes = np.array(period_sharpes)
        
        # Stabilit√© = 1 - coefficient de variation des Sharpe ratios
        sharpe_mean = np.mean(period_sharpes)
        sharpe_std = np.std(period_sharpes)
        
        if abs(sharpe_mean) > 1e-8:
            cv = sharpe_std / abs(sharpe_mean)
            stability_score = max(0.0, 1.0 - cv)
        else:
            stability_score = 0.0
        
        # Consistance = corr√©lation rang des performances
        period_ranks = stats.rankdata(period_sharpes)
        expected_ranks = np.arange(1, len(period_sharpes) + 1)
        consistency = stats.spearmanr(period_ranks, expected_ranks)[0]
        consistency = max(0.0, consistency) if not np.isnan(consistency) else 0.0
        
        # Pourcentage de p√©riodes positives
        periods_positive = (period_sharpes > 0).mean()
        
        return {
            "stability_score": float(stability_score),
            "sharpe_consistency": float(consistency),
            "periods_positive": float(periods_positive),
            "period_sharpes": period_sharpes.tolist(),
            "n_periods": len(period_sharpes)
        }
    
    def bootstrap_psr_validation(self, returns: np.ndarray, 
                                 n_bootstrap: int = 1000) -> Dict[str, float]:
        """
        Validation bootstrap de la robustesse du PSR
        """
        
        returns = np.asarray(returns)
        
        if len(returns) < 50:  # Minimum pour bootstrap
            return {
                "bootstrap_psr_mean": 0.0,
                "bootstrap_psr_std": np.inf,
                "bootstrap_confidence_interval": (0.0, 0.0)
            }
        
        bootstrap_psrs = []
        
        for _ in range(n_bootstrap):
            # √âchantillonnage avec remise
            boot_returns = np.random.choice(returns, size=len(returns), replace=True)
            
            # PSR sur l'√©chantillon bootstrap
            boot_psr_result = self.calculate_precise_psr(boot_returns)
            bootstrap_psrs.append(boot_psr_result["psr"])
        
        bootstrap_psrs = np.array(bootstrap_psrs)
        
        # Statistiques bootstrap
        boot_mean = np.mean(bootstrap_psrs)
        boot_std = np.std(bootstrap_psrs)
        
        # Intervalle de confiance bootstrap
        alpha = 1 - self.confidence_level
        boot_lower = np.percentile(bootstrap_psrs, 100 * alpha / 2)
        boot_upper = np.percentile(bootstrap_psrs, 100 * (1 - alpha / 2))
        
        return {
            "bootstrap_psr_mean": float(boot_mean),
            "bootstrap_psr_std": float(boot_std),
            "bootstrap_confidence_interval": (float(boot_lower), float(boot_upper)),
            "bootstrap_psrs": bootstrap_psrs.tolist()
        }
    
    def analyze_alpha_correlation(self, alpha_returns_dict: Dict[str, np.ndarray]) -> Dict:
        """
        Analyse des corr√©lations entre alphas pour diversification
        """
        
        if len(alpha_returns_dict) < 2:
            return {"correlation_matrix": {}, "diversification_ratio": 1.0}
        
        # Matrice des returns align√©s
        alpha_names = list(alpha_returns_dict.keys())
        min_length = min(len(rets) for rets in alpha_returns_dict.values())
        
        returns_matrix = np.zeros((min_length, len(alpha_names)))
        
        for i, alpha_name in enumerate(alpha_names):
            returns_matrix[:, i] = alpha_returns_dict[alpha_name][:min_length]
        
        # Matrice de corr√©lation
        corr_matrix = np.corrcoef(returns_matrix.T)
        
        # Conversion en dictionnaire pour s√©rialisation
        corr_dict = {}
        for i, name_i in enumerate(alpha_names):
            corr_dict[name_i] = {}
            for j, name_j in enumerate(alpha_names):
                corr_dict[name_i][name_j] = float(corr_matrix[i, j])
        
        # Ratio de diversification
        # DR = œÉ(portfolio √©gal weight) / moyenne pond√©r√©e des œÉ individuelles
        portfolio_weights = np.ones(len(alpha_names)) / len(alpha_names)
        
        individual_vols = np.std(returns_matrix, axis=0)
        portfolio_vol = np.std(returns_matrix @ portfolio_weights)
        weighted_avg_vol = portfolio_weights @ individual_vols
        
        diversification_ratio = weighted_avg_vol / (portfolio_vol + 1e-8)
        
        return {
            "correlation_matrix": corr_dict,
            "diversification_ratio": float(diversification_ratio),
            "avg_correlation": float(np.mean(corr_matrix[np.triu_indices_from(corr_matrix, k=1)])),
            "max_correlation": float(np.max(corr_matrix[np.triu_indices_from(corr_matrix, k=1)])),
            "min_correlation": float(np.min(corr_matrix[np.triu_indices_from(corr_matrix, k=1)]))
        }
    
    def load_alpha_metrics(self, artifacts_dir: str = "data/artifacts") -> Dict[str, Dict]:
        """
        Charge les m√©triques de tous les alphas disponibles
        """
        
        artifacts_path = Path(artifacts_dir)
        if not artifacts_path.exists():
            logger.warning(f"‚ö†Ô∏è  Dossier artifacts inexistant: {artifacts_dir}")
            return {}
        
        alpha_metrics = {}
        metric_files = list(artifacts_path.glob("*_metrics_*.json"))
        
        logger.info(f"üìä Chargement de {len(metric_files)} fichiers de m√©triques...")
        
        for metric_file in metric_files:
            try:
                with open(metric_file, 'r') as f:
                    metrics = json.load(f)
                
                # Extraction du nom d'alpha depuis le nom de fichier
                filename_parts = metric_file.stem.split('_')
                if len(filename_parts) >= 2:
                    alpha_name = filename_parts[0]  # dmn, mr, funding, etc.
                    symbol = filename_parts[-1] if len(filename_parts) > 2 else "UNKNOWN"
                    
                    full_alpha_name = f"{alpha_name}_{symbol}"
                    alpha_metrics[full_alpha_name] = metrics
                    
                    logger.info(f"‚úÖ M√©triques charg√©es: {full_alpha_name}")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur chargement {metric_file}: {e}")
                continue
        
        return alpha_metrics
    
    def calculate_composite_score(self, metrics: Dict[str, float]) -> float:
        """
        Score composite multi-crit√®res avec pond√©ration adaptative
        """
        
        # Poids des crit√®res (ajustables)
        weights = {
            "psr": 0.35,           # Principal crit√®re
            "sharpe": 0.25,        # Performance ajust√©e risque
            "stability": 0.15,     # Stabilit√© temporelle
            "maxdd": 0.15,         # Contr√¥le du risque
            "diversification": 0.10 # B√©n√©fice diversification
        }
        
        # Normalisation des m√©triques (0-1)
        normalized_scores = {}
        
        # PSR (d√©j√† entre 0 et 1)
        normalized_scores["psr"] = min(1.0, max(0.0, metrics.get("psr", 0.0)))
        
        # Sharpe (normalisation sigmo√Øde)
        sharpe = metrics.get("sharpe_ratio", 0.0)
        normalized_scores["sharpe"] = 1 / (1 + np.exp(-sharpe))  # Sigmoid
        
        # Stabilit√© (d√©j√† entre 0 et 1 si calcul√©e)
        normalized_scores["stability"] = metrics.get("stability_score", 0.5)
        
        # Max Drawdown (inverser car plus petit = mieux)
        maxdd = metrics.get("max_drawdown", 1.0)
        normalized_scores["maxdd"] = max(0.0, 1.0 - min(1.0, maxdd / 0.5))
        
        # Diversification (si disponible)
        normalized_scores["diversification"] = metrics.get("diversification_benefit", 0.5)
        
        # Score composite pond√©r√©
        composite_score = sum(
            weights[criterion] * normalized_scores[criterion]
            for criterion in weights.keys()
        )
        
        return float(composite_score)
    
    def select_alphas(self, 
                     artifacts_dir: str = "data/artifacts",
                     max_selected: int = 5,
                     force_diversification: bool = True) -> Dict:
        """
        S√©lection des alphas selon crit√®res multi-dimensionnels
        """
        
        logger.info("üéØ D√©but s√©lection des alphas avec PSR...")
        
        # 1. Chargement des m√©triques
        alpha_metrics = self.load_alpha_metrics(artifacts_dir)
        
        if not alpha_metrics:
            logger.error("‚ùå Aucune m√©trique d'alpha trouv√©e")
            return {
                "selected_alphas": [],
                "rejected_alphas": [],
                "selection_summary": {}
            }
        
        # 2. Filtrage basique
        candidates = {}
        rejected = {}
        
        for alpha_name, metrics in alpha_metrics.items():
            
            # Crit√®res de base
            psr = metrics.get("psr", 0.0)
            sharpe = metrics.get("sharpe_ratio", 0.0)
            maxdd = metrics.get("max_drawdown", 1.0)
            
            # Tests de seuils
            passes_psr = psr >= self.psr_threshold
            passes_sharpe = sharpe >= self.sharpe_threshold  
            passes_maxdd = maxdd <= self.maxdd_threshold
            
            if passes_psr and passes_sharpe and passes_maxdd:
                # Calcul score composite
                composite_score = self.calculate_composite_score(metrics)
                
                candidates[alpha_name] = {
                    **metrics,
                    "composite_score": composite_score,
                    "selection_reason": "Passes all thresholds"
                }
                
                logger.info(f"‚úÖ Candidat: {alpha_name} (PSR={psr:.3f}, Sharpe={sharpe:.3f})")
                
            else:
                reasons = []
                if not passes_psr:
                    reasons.append(f"PSR trop faible ({psr:.3f} < {self.psr_threshold})")
                if not passes_sharpe:
                    reasons.append(f"Sharpe trop faible ({sharpe:.3f} < {self.sharpe_threshold})")
                if not passes_maxdd:
                    reasons.append(f"DrawDown trop √©lev√© ({maxdd:.3f} > {self.maxdd_threshold})")
                
                rejected[alpha_name] = {
                    **metrics,
                    "rejection_reason": " | ".join(reasons)
                }
                
                logger.info(f"‚ùå Rejet√©: {alpha_name} - {rejected[alpha_name]['rejection_reason']}")
        
        # 3. S√©lection finale par score composite
        if not candidates:
            logger.warning("‚ö†Ô∏è  Aucun candidat ne passe les seuils de base")
            return {
                "selected_alphas": [],
                "rejected_alphas": list(rejected.keys()),
                "selection_summary": {
                    "total_evaluated": len(alpha_metrics),
                    "candidates": 0,
                    "selected": 0
                }
            }
        
        # Tri par score composite
        sorted_candidates = sorted(
            candidates.items(),
            key=lambda x: x[1]["composite_score"],
            reverse=True
        )
        
        # S√©lection avec contrainte de diversification
        selected_alphas = []
        
        for alpha_name, alpha_data in sorted_candidates:
            if len(selected_alphas) >= max_selected:
                break
            
            # TODO: Impl√©menter contrainte de corr√©lation si force_diversification
            selected_alphas.append((alpha_name, alpha_data))
        
        # 4. R√©sum√© de s√©lection
        selection_summary = {
            "total_evaluated": len(alpha_metrics),
            "candidates": len(candidates), 
            "selected": len(selected_alphas),
            "selection_criteria": {
                "psr_threshold": self.psr_threshold,
                "sharpe_threshold": self.sharpe_threshold,
                "maxdd_threshold": self.maxdd_threshold
            },
            "best_composite_score": selected_alphas[0][1]["composite_score"] if selected_alphas else 0.0
        }
        
        logger.info(f"üìä S√âLECTION TERMIN√âE:")
        logger.info(f"   - √âvalu√©s: {selection_summary['total_evaluated']}")
        logger.info(f"   - Candidats: {selection_summary['candidates']}")
        logger.info(f"   - S√©lectionn√©s: {selection_summary['selected']}")
        
        return {
            "selected_alphas": [{"name": name, "metrics": data} for name, data in selected_alphas],
            "rejected_alphas": list(rejected.keys()),
            "rejection_reasons": {name: data["rejection_reason"] for name, data in rejected.items()},
            "selection_summary": selection_summary
        }
    
    def save_selection_results(self, selection_results: Dict, 
                              output_path: str = "data/artifacts/selection_results.json"):
        """
        Sauvegarde les r√©sultats de s√©lection
        """
        
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Ajout timestamp
        selection_results["timestamp"] = datetime.now().isoformat()
        selection_results["selector_config"] = {
            "psr_threshold": self.psr_threshold,
            "sharpe_threshold": self.sharpe_threshold,
            "maxdd_threshold": self.maxdd_threshold,
            "confidence_level": self.confidence_level
        }
        
        with open(output_file, 'w') as f:
            json.dump(selection_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ R√©sultats de s√©lection sauvegard√©s: {output_file}")

# ==============================================
# FONCTION PRINCIPALE DE S√âLECTION
# ==============================================

def run_alpha_selection(artifacts_dir: str = "data/artifacts",
                       config: Dict = None) -> Dict:
    """
    Fonction principale de s√©lection des alphas
    
    Args:
        artifacts_dir: Dossier contenant les m√©triques des alphas
        config: Configuration du s√©lecteur
    """
    
    # Configuration par d√©faut
    if config is None:
        config = {
            "psr_threshold": 0.65,
            "sharpe_threshold": 1.0,
            "maxdd_threshold": 0.25,
            "max_selected": 5,
            "confidence_level": 0.95
        }
    
    logger.info("üöÄ D√©but s√©lection PSR des alphas")
    
    # MLflow tracking
    mlflow.set_experiment("AlphaSelection_PSR")
    
    with mlflow.start_run():
        
        # Log config
        mlflow.log_params(config)
        
        # S√©lecteur
        selector = PSRSelector(
            psr_threshold=config["psr_threshold"],
            sharpe_threshold=config["sharpe_threshold"],
            maxdd_threshold=config["maxdd_threshold"],
            confidence_level=config["confidence_level"]
        )
        
        # S√©lection
        results = selector.select_alphas(
            artifacts_dir=artifacts_dir,
            max_selected=config["max_selected"]
        )
        
        # Log m√©triques MLflow
        summary = results["selection_summary"]
        mlflow.log_metrics({
            "total_evaluated": summary["total_evaluated"],
            "candidates": summary["candidates"],
            "selected": summary["selected"],
            "selection_rate": summary["selected"] / max(1, summary["total_evaluated"])
        })
        
        # Sauvegarde
        selector.save_selection_results(results)
        
        # Log artifact MLflow
        mlflow.log_artifact("data/artifacts/selection_results.json")
    
    return results

# ==============================================
# SCRIPT PRINCIPAL
# ==============================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="S√©lection PSR des alphas")
    parser.add_argument("--artifacts-dir", default="data/artifacts",
                       help="Dossier des m√©triques d'alphas")
    parser.add_argument("--psr-threshold", type=float, default=0.65)
    parser.add_argument("--sharpe-threshold", type=float, default=1.0)  
    parser.add_argument("--maxdd-threshold", type=float, default=0.25)
    parser.add_argument("--max-selected", type=int, default=5)
    
    args = parser.parse_args()
    
    config = {
        "psr_threshold": args.psr_threshold,
        "sharpe_threshold": args.sharpe_threshold,
        "maxdd_threshold": args.maxdd_threshold,
        "max_selected": args.max_selected
    }
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        results = run_alpha_selection(args.artifacts_dir, config)
        
        print("\n" + "="*60)
        print("R√âSULTATS S√âLECTION PSR")
        print("="*60)
        
        for alpha_info in results["selected_alphas"]:
            name = alpha_info["name"]
            metrics = alpha_info["metrics"]
            print(f"\n‚úÖ {name}:")
            print(f"   PSR: {metrics.get('psr', 0):.3f}")
            print(f"   Sharpe: {metrics.get('sharpe_ratio', 0):.3f}")
            print(f"   Max DD: {metrics.get('max_drawdown', 0):.3f}")
            print(f"   Score Composite: {metrics.get('composite_score', 0):.3f}")
        
        logger.info("‚úÖ S√©lection PSR termin√©e avec succ√®s")
        
    except Exception as e:
        logger.error(f"‚ùå ERREUR s√©lection PSR: {e}")
        sys.exit(1)