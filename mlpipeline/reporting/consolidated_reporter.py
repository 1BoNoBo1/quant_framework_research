#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Consolidated Reporter - Rapport unifi√© pipeline + validation
Consolide tous les r√©sultats (pipeline, validation, overfitting) en un rapport final
"""

import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import glob

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ConsolidatedReporter:
    """G√©n√®re un rapport consolid√© pipeline + validation institutionnelle"""

    def __init__(self, output_dir: str = "logs"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.report_data = {
            "metadata": {
                "report_type": "consolidated",
                "generation_timestamp": datetime.now().isoformat(),
                "framework_version": "1.0.0"
            },
            "pipeline": {},
            "validation": {},
            "recommendations": [],
            "summary": {}
        }

    def load_latest_pipeline_report(self) -> Optional[Dict[str, Any]]:
        """Charge le dernier rapport de pipeline"""
        try:
            pattern = str(self.output_dir / "hybrid_pipeline_report_*.json")
            files = glob.glob(pattern)
            if not files:
                logger.warning("Aucun rapport pipeline trouv√©")
                return None

            latest_file = max(files, key=os.path.getctime)
            logger.info(f"üìä Chargement pipeline: {latest_file}")

            with open(latest_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement pipeline: {e}")
            return None

    def load_validation_results(self) -> Dict[str, Any]:
        """Charge les r√©sultats de validation"""
        validation_data = {
            "oos_validation": None,
            "walk_forward": None,
            "overfitting": None,
            "psr_selection": None
        }

        # OOS Validation
        oos_files = ["test_oos_results.json", "oos_validation_results.json"]
        for oos_file in oos_files:
            if os.path.exists(oos_file):
                try:
                    with open(oos_file, 'r') as f:
                        validation_data["oos_validation"] = json.load(f)
                        logger.info(f"üìà OOS validation charg√©e: {oos_file}")
                        break
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erreur OOS: {e}")

        # PSR Selection
        psr_file = "data/artifacts/psr_selection_results.json"
        if os.path.exists(psr_file):
            try:
                with open(psr_file, 'r') as f:
                    validation_data["psr_selection"] = json.load(f)
                    logger.info(f"üéØ PSR selection charg√©e")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur PSR: {e}")

        return validation_data

    def analyze_pipeline_health(self, pipeline_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyse la sant√© du pipeline"""
        if not pipeline_data:
            return {"status": "MISSING", "issues": ["Pipeline data not found"]}

        summary = pipeline_data.get("summary", {})
        success_rate = summary.get("success_rate", 0)
        execution_time = pipeline_data.get("execution_time", 0)

        issues = []
        if success_rate < 1.0:
            issues.append(f"Taux de succ√®s: {success_rate*100:.1f}% (cible: 100%)")
        if execution_time > 300:  # 5 minutes
            issues.append(f"Temps d'ex√©cution √©lev√©: {execution_time:.1f}s")

        status = "HEALTHY" if not issues else "DEGRADED" if success_rate > 0.8 else "CRITICAL"

        return {
            "status": status,
            "success_rate": success_rate,
            "execution_time": execution_time,
            "total_tasks": summary.get("total_tasks", 0),
            "issues": issues
        }

    def analyze_validation_health(self, validation_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analyse la sant√© de la validation"""
        issues = []
        scores = {}

        # OOS Analysis
        oos = validation_data.get("oos_validation")
        if oos:
            oos_sharpe = oos.get("performance", {}).get("out_of_sample", {}).get("sharpe_ratio", 0)
            scores["oos_sharpe"] = oos_sharpe
            if oos_sharpe <= 0:
                issues.append("Sharpe OOS ‚â§ 0 (strat√©gies non viables)")
        else:
            issues.append("Validation OOS manquante")

        # Determine overall validation status
        if not validation_data.get("oos_validation"):
            status = "MISSING"
        elif scores.get("oos_sharpe", 0) > 0.5:
            status = "GOOD"
        elif scores.get("oos_sharpe", 0) > 0:
            status = "WEAK"
        else:
            status = "FAILED"

        return {
            "status": status,
            "scores": scores,
            "issues": issues
        }

    def generate_recommendations(self, pipeline_health: Dict, validation_health: Dict) -> List[str]:
        """G√©n√®re des recommandations bas√©es sur l'analyse"""
        recommendations = []

        # Pipeline recommendations
        if pipeline_health["status"] != "HEALTHY":
            recommendations.append("üîß Optimiser la stabilit√© du pipeline")
            if pipeline_health.get("execution_time", 0) > 300:
                recommendations.append("‚ö° Am√©liorer les performances d'ex√©cution")

        # Validation recommendations
        val_status = validation_health["status"]
        if val_status == "FAILED":
            recommendations.extend([
                "üìà Revoir compl√®tement les strat√©gies alpha (performance OOS nulle)",
                "üéØ R√©duire l'overfitting (optimisation excessive)",
                "üìä Augmenter la p√©riode d'entra√Ænement"
            ])
        elif val_status == "WEAK":
            recommendations.extend([
                "üîß Optimiser les hyperparam√®tres des strat√©gies",
                "üìà Am√©liorer la robustesse out-of-sample",
                "üé≤ R√©duire le multiple testing bias"
            ])

        # Cross-cutting recommendations
        if pipeline_health["success_rate"] == 1.0 and val_status in ["WEAK", "FAILED"]:
            recommendations.append("üèóÔ∏è Pipeline stable mais strat√©gies √† am√©liorer")

        return recommendations

    def generate_executive_summary(self, pipeline_health: Dict, validation_health: Dict) -> Dict[str, Any]:
        """G√©n√®re un r√©sum√© ex√©cutif"""
        pipeline_status = pipeline_health["status"]
        validation_status = validation_health["status"]

        # Overall status logic
        if pipeline_status == "HEALTHY" and validation_status == "GOOD":
            overall_status = "PRODUCTION_READY"
            overall_message = "‚úÖ Syst√®me pr√™t pour la production"
        elif pipeline_status == "HEALTHY" and validation_status in ["WEAK", "FAILED"]:
            overall_status = "NEEDS_OPTIMIZATION"
            overall_message = "üü° Infrastructure stable, strat√©gies √† optimiser"
        elif pipeline_status != "HEALTHY":
            overall_status = "INFRASTRUCTURE_ISSUES"
            overall_message = "üîß Probl√®mes d'infrastructure √† r√©soudre"
        else:
            overall_status = "CRITICAL"
            overall_message = "‚ùå Syst√®me non viable - r√©vision compl√®te n√©cessaire"

        return {
            "overall_status": overall_status,
            "message": overall_message,
            "pipeline_health": pipeline_status,
            "validation_health": validation_status,
            "readiness_score": self._calculate_readiness_score(pipeline_health, validation_health)
        }

    def _calculate_readiness_score(self, pipeline_health: Dict, validation_health: Dict) -> float:
        """Calcule un score de maturit√© globale (0-1)"""
        pipeline_score = pipeline_health.get("success_rate", 0) * 0.4

        val_scores = validation_health.get("scores", {})
        validation_score = max(0, val_scores.get("oos_sharpe", 0)) * 0.6

        return min(1.0, pipeline_score + validation_score)

    def consolidate_report(self) -> str:
        """G√©n√®re le rapport consolid√© complet"""
        logger.info("üèóÔ∏è G√©n√©ration rapport consolid√©...")

        # Chargement des donn√©es
        pipeline_data = self.load_latest_pipeline_report()
        validation_data = self.load_validation_results()

        # Analyse
        pipeline_health = self.analyze_pipeline_health(pipeline_data)
        validation_health = self.analyze_validation_health(validation_data)

        # Construction du rapport
        self.report_data["pipeline"] = {
            "data": pipeline_data,
            "health": pipeline_health
        }

        self.report_data["validation"] = {
            "data": validation_data,
            "health": validation_health
        }

        self.report_data["recommendations"] = self.generate_recommendations(
            pipeline_health, validation_health
        )

        self.report_data["summary"] = self.generate_executive_summary(
            pipeline_health, validation_health
        )

        # Sauvegarde
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.output_dir / f"consolidated_report_{timestamp}.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, indent=2, ensure_ascii=False)

        # Affichage r√©sum√©
        self._display_summary()

        logger.info(f"üìä Rapport consolid√© sauv√©: {output_file}")
        return str(output_file)

    def _display_summary(self):
        """Affiche le r√©sum√© du rapport"""
        summary = self.report_data["summary"]
        pipeline_health = self.report_data["pipeline"]["health"]
        validation_health = self.report_data["validation"]["health"]

        print(f"\n{'-'*60}")
        print("üìä RAPPORT CONSOLID√â QUANTITATIF")
        print(f"{'-'*60}")

        print(f"\nüéØ STATUT GLOBAL: {summary['message']}")
        print(f"üìà Score de maturit√©: {summary['readiness_score']:.1%}")

        print(f"\nüèóÔ∏è INFRASTRUCTURE PIPELINE:")
        print(f"   Status: {pipeline_health['status']}")
        print(f"   Taux succ√®s: {pipeline_health.get('success_rate', 0):.1%}")
        print(f"   Temps ex√©cution: {pipeline_health.get('execution_time', 0):.1f}s")

        print(f"\nüõ°Ô∏è VALIDATION INSTITUTIONNELLE:")
        print(f"   Status: {validation_health['status']}")
        scores = validation_health.get('scores', {})
        if 'oos_sharpe' in scores:
            print(f"   Sharpe OOS: {scores['oos_sharpe']:.3f}")

        print(f"\nüîß RECOMMANDATIONS:")
        for i, rec in enumerate(self.report_data["recommendations"][:5], 1):
            print(f"   {i}. {rec}")

        print(f"\n{'-'*60}")

def main():
    """Point d'entr√©e principal"""
    try:
        reporter = ConsolidatedReporter()
        report_file = reporter.consolidate_report()

        print(f"\n‚úÖ Rapport consolid√© g√©n√©r√©: {report_file}")
        return 0

    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©ration rapport: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())