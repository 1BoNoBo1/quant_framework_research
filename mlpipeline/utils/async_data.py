#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Async Data Management - Gestion asynchrone des donnÃ©es
Optimise les opÃ©rations I/O pour le pipeline ML
"""

import asyncio
import aiofiles
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import json

logger = logging.getLogger(__name__)

class AsyncDataManager:
    """
    Gestionnaire de donnÃ©es asynchrone pour optimiser les I/O
    """
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.thread_executor = ThreadPoolExecutor(max_workers=max_workers)
        self.process_executor = ProcessPoolExecutor(max_workers=max_workers)
        self._cache = {}
    
    async def read_parquet_async(self, file_path: str) -> pd.DataFrame:
        """
        Lecture async d'un fichier parquet
        """
        if file_path in self._cache:
            logger.debug(f"ðŸ“‹ Cache hit: {file_path}")
            return self._cache[file_path].copy()
        
        logger.info(f"ðŸ“Š Loading parquet async: {file_path}")
        
        loop = asyncio.get_event_loop()
        df = await loop.run_in_executor(
            self.thread_executor,
            pd.read_parquet,
            file_path
        )
        
        # Cache avec limite de taille
        if len(self._cache) < 10:  # Limite Ã  10 fichiers en cache
            self._cache[file_path] = df.copy()
        
        logger.info(f"âœ… Loaded: {len(df)} rows from {Path(file_path).name}")
        return df
    
    async def write_parquet_async(self, df: pd.DataFrame, file_path: str) -> None:
        """
        Ã‰criture async d'un fichier parquet
        """
        logger.info(f"ðŸ’¾ Saving parquet async: {file_path}")
        
        # CrÃ©er le rÃ©pertoire si nÃ©cessaire
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)
        
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            self.thread_executor,
            lambda: df.to_parquet(file_path, compression='snappy')
        )
        
        logger.info(f"âœ… Saved: {len(df)} rows to {Path(file_path).name}")
    
    async def read_json_async(self, file_path: str) -> Dict:
        """
        Lecture async d'un fichier JSON
        """
        logger.debug(f"ðŸ“„ Loading JSON async: {file_path}")
        
        async with aiofiles.open(file_path, 'r') as f:
            content = await f.read()
            return json.loads(content)
    
    async def write_json_async(self, data: Dict, file_path: str) -> None:
        """
        Ã‰criture async d'un fichier JSON
        """
        logger.debug(f"ðŸ’¾ Saving JSON async: {file_path}")
        
        # CrÃ©er le rÃ©pertoire si nÃ©cessaire
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)
        
        async with aiofiles.open(file_path, 'w') as f:
            await f.write(json.dumps(data, indent=2))
    
    async def load_multiple_parquets(self, file_paths: List[str]) -> List[pd.DataFrame]:
        """
        Chargement parallÃ¨le de plusieurs fichiers parquet
        """
        logger.info(f"ðŸ“Š Loading {len(file_paths)} parquet files in parallel")
        
        tasks = [self.read_parquet_async(path) for path in file_paths]
        dfs = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filtrer les erreurs
        valid_dfs = []
        for i, result in enumerate(dfs):
            if isinstance(result, Exception):
                logger.error(f"âŒ Failed to load {file_paths[i]}: {result}")
            else:
                valid_dfs.append(result)
        
        logger.info(f"âœ… Successfully loaded {len(valid_dfs)}/{len(file_paths)} files")
        return valid_dfs
    
    async def save_multiple_parquets(self, df_path_pairs: List[Tuple[pd.DataFrame, str]]) -> None:
        """
        Sauvegarde parallÃ¨le de plusieurs fichiers parquet
        """
        logger.info(f"ðŸ’¾ Saving {len(df_path_pairs)} parquet files in parallel")
        
        tasks = [
            self.write_parquet_async(df, path) 
            for df, path in df_path_pairs
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Compter les succÃ¨s
        successes = sum(1 for r in results if not isinstance(r, Exception))
        errors = sum(1 for r in results if isinstance(r, Exception))
        
        if errors > 0:
            logger.warning(f"âš ï¸ {errors} files failed to save")
        
        logger.info(f"âœ… Successfully saved {successes}/{len(df_path_pairs)} files")
    
    async def process_dataframe_async(self, 
                                    df: pd.DataFrame, 
                                    processing_func,
                                    *args, **kwargs) -> pd.DataFrame:
        """
        Traitement async d'un DataFrame avec fonction CPU-intensive
        """
        logger.debug(f"âš™ï¸ Processing dataframe async: {len(df)} rows")
        
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.process_executor,
            processing_func,
            df,
            *args
        )
        
        logger.debug(f"âœ… Processing completed: {len(result)} rows")
        return result
    
    async def validate_data_async(self, df: pd.DataFrame, source: str = "unknown") -> bool:
        """
        Validation asynchrone des donnÃ©es
        """
        logger.debug(f"ðŸ” Validating data async: {source}")
        
        try:
            # Import validation function
            from mlpipeline.utils.artifact_cleaner import validate_real_data_only
            
            loop = asyncio.get_event_loop()
            is_valid = await loop.run_in_executor(
                self.thread_executor,
                validate_real_data_only,
                df,
                source
            )
            
            if is_valid:
                logger.debug(f"âœ… Data validation passed: {source}")
            else:
                logger.warning(f"âš ï¸ Data validation failed: {source}")
            
            return is_valid
            
        except ImportError:
            logger.debug("ðŸ“‹ Validation function not available, skipping")
            return True
        except Exception as e:
            logger.error(f"âŒ Validation error for {source}: {e}")
            return False
    
    async def cleanup(self):
        """
        Nettoyage des ressources async
        """
        logger.info("ðŸ§¹ Cleaning up async resources")
        
        self.thread_executor.shutdown(wait=False)
        self.process_executor.shutdown(wait=False)
        self._cache.clear()
        
        logger.info("âœ… Async cleanup completed")


class AsyncFeatureProcessor:
    """
    Processeur de features asynchrone
    """
    
    def __init__(self, data_manager: AsyncDataManager):
        self.data_manager = data_manager
    
    async def process_features_batch(self, 
                                   input_files: List[str],
                                   output_dir: str,
                                   processing_func) -> List[str]:
        """
        Traitement par batch des features en parallÃ¨le
        """
        logger.info(f"ðŸ”„ Processing {len(input_files)} feature files in batch")
        
        # Chargement parallÃ¨le
        dfs = await self.data_manager.load_multiple_parquets(input_files)
        
        # Traitement parallÃ¨le
        tasks = []
        output_files = []
        
        for i, df in enumerate(dfs):
            input_name = Path(input_files[i]).stem
            output_path = f"{output_dir}/processed_{input_name}.parquet"
            output_files.append(output_path)
            
            # Traitement async
            task = self.data_manager.process_dataframe_async(df, processing_func)
            tasks.append(task)
        
        processed_dfs = await asyncio.gather(*tasks)
        
        # Sauvegarde parallÃ¨le
        save_pairs = list(zip(processed_dfs, output_files))
        await self.data_manager.save_multiple_parquets(save_pairs)
        
        logger.info(f"âœ… Batch processing completed: {len(output_files)} files")
        return output_files


async def create_async_data_pipeline(config: Dict) -> AsyncDataManager:
    """
    Factory pour crÃ©er un pipeline de donnÃ©es async
    """
    max_workers = config.get('max_workers', 4)
    
    logger.info(f"ðŸš€ Creating async data pipeline (workers={max_workers})")
    
    data_manager = AsyncDataManager(max_workers=max_workers)
    
    return data_manager


async def example_usage():
    """
    Exemple d'utilisation du systÃ¨me async
    """
    logger.info("ðŸ”„ Demo async data management")
    
    # Configuration
    config = {'max_workers': 4}
    
    # CrÃ©ation du manager
    data_manager = await create_async_data_pipeline(config)
    
    try:
        # Exemple: chargement parallÃ¨le
        files = [
            "data/processed/features_BTCUSDT_1h.parquet",
            "data/processed/features_ETHUSDT_1h.parquet"
        ]
        
        # Filtrer les fichiers existants
        existing_files = [f for f in files if Path(f).exists()]
        
        if existing_files:
            dfs = await data_manager.load_multiple_parquets(existing_files)
            logger.info(f"ðŸ“Š Loaded {len(dfs)} dataframes")
            
            # Validation parallÃ¨le
            validation_tasks = [
                data_manager.validate_data_async(df, f"file_{i}") 
                for i, df in enumerate(dfs)
            ]
            validations = await asyncio.gather(*validation_tasks)
            
            valid_count = sum(validations)
            logger.info(f"âœ… {valid_count}/{len(dfs)} files passed validation")
        
        else:
            logger.info("ðŸ“‹ No files found for demo")
    
    finally:
        # Nettoyage
        await data_manager.cleanup()


if __name__ == "__main__":
    import logging
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Lancer la dÃ©mo
    asyncio.run(example_usage())