#!/usr/bin/env python3
"""
GÃ©nÃ©rateur automatique de documentation benchmarks pour QFrame.
GÃ©nÃ¨re des rapports de performance et des mÃ©triques depuis les tests.
"""

import json
import sys
import time
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import subprocess
import statistics

# Ajouter le chemin du projet
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

def run_performance_tests() -> Dict[str, Any]:
    """ExÃ©cute les tests de performance et collecte les mÃ©triques."""
    results = {
        "timestamp": datetime.now().isoformat(),
        "framework_version": "0.1.0",
        "python_version": sys.version,
        "benchmarks": {}
    }

    # Test d'import du framework
    start_time = time.time()
    try:
        from qframe.core.container import get_container
        from qframe.core.config import get_config
        import_time = time.time() - start_time
        results["benchmarks"]["framework_import"] = {
            "time_seconds": import_time,
            "status": "success",
            "description": "Temps d'import du framework core"
        }
    except Exception as e:
        results["benchmarks"]["framework_import"] = {
            "time_seconds": time.time() - start_time,
            "status": "error",
            "error": str(e),
            "description": "Temps d'import du framework core"
        }

    # Test de crÃ©ation du container DI
    start_time = time.time()
    try:
        container = get_container()
        container_time = time.time() - start_time
        results["benchmarks"]["di_container_creation"] = {
            "time_seconds": container_time,
            "status": "success",
            "description": "Temps de crÃ©ation du container DI"
        }
    except Exception as e:
        results["benchmarks"]["di_container_creation"] = {
            "time_seconds": time.time() - start_time,
            "status": "error",
            "error": str(e),
            "description": "Temps de crÃ©ation du container DI"
        }

    # Test de chargement configuration
    start_time = time.time()
    try:
        config = get_config()
        config_time = time.time() - start_time
        results["benchmarks"]["config_loading"] = {
            "time_seconds": config_time,
            "status": "success",
            "description": "Temps de chargement configuration"
        }
    except Exception as e:
        results["benchmarks"]["config_loading"] = {
            "time_seconds": time.time() - start_time,
            "status": "error",
            "error": str(e),
            "description": "Temps de chargement configuration"
        }

    # Test des stratÃ©gies (si disponibles)
    try:
        from qframe.strategies.research.adaptive_mean_reversion_strategy import AdaptiveMeanReversionStrategy

        start_time = time.time()
        strategy = container.resolve(AdaptiveMeanReversionStrategy)
        strategy_time = time.time() - start_time

        results["benchmarks"]["strategy_resolution"] = {
            "time_seconds": strategy_time,
            "status": "success",
            "description": "Temps de rÃ©solution stratÃ©gie Mean Reversion"
        }
    except Exception as e:
        results["benchmarks"]["strategy_resolution"] = {
            "time_seconds": 0,
            "status": "error",
            "error": str(e),
            "description": "Temps de rÃ©solution stratÃ©gie Mean Reversion"
        }

    return results

def run_memory_benchmarks() -> Dict[str, Any]:
    """Mesure l'utilisation mÃ©moire du framework."""
    import psutil
    import os

    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss

    memory_results = {
        "initial_memory_mb": initial_memory / (1024 * 1024),
        "benchmarks": {}
    }

    # Mesurer mÃ©moire aprÃ¨s import
    try:
        from qframe.core.container import get_container
        from qframe.core.config import get_config

        after_import_memory = process.memory_info().rss
        memory_results["benchmarks"]["after_imports"] = {
            "memory_mb": after_import_memory / (1024 * 1024),
            "delta_mb": (after_import_memory - initial_memory) / (1024 * 1024),
            "description": "MÃ©moire aprÃ¨s imports core"
        }

        # Mesurer mÃ©moire aprÃ¨s crÃ©ation container
        container = get_container()
        after_container_memory = process.memory_info().rss
        memory_results["benchmarks"]["after_container"] = {
            "memory_mb": after_container_memory / (1024 * 1024),
            "delta_mb": (after_container_memory - after_import_memory) / (1024 * 1024),
            "description": "MÃ©moire aprÃ¨s crÃ©ation container"
        }

    except Exception as e:
        memory_results["error"] = str(e)

    return memory_results

def generate_benchmark_report(perf_results: Dict, memory_results: Dict) -> str:
    """GÃ©nÃ¨re le rapport de benchmarks en Markdown."""

    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    content = f"""# ğŸ“Š Benchmarks Performance QFrame

**DerniÃ¨re mise Ã  jour** : {timestamp}

## MÃ©triques de Performance

### Temps d'ExÃ©cution

| Benchmark | Temps (ms) | Statut | Description |
|-----------|------------|--------|-------------|
"""

    for name, result in perf_results.get("benchmarks", {}).items():
        time_ms = round(result.get("time_seconds", 0) * 1000, 2)
        status = "âœ…" if result.get("status") == "success" else "âŒ"
        description = result.get("description", "")
        content += f"| {name} | {time_ms} | {status} | {description} |\n"

    content += "\n### MÃ©triques MÃ©moire\n\n"

    if "error" not in memory_results:
        content += f"**MÃ©moire initiale** : {memory_results['initial_memory_mb']:.2f} MB\n\n"
        content += "| Ã‰tape | MÃ©moire (MB) | Delta (MB) | Description |\n"
        content += "|-------|--------------|------------|-------------|\n"

        for name, result in memory_results.get("benchmarks", {}).items():
            memory_mb = round(result.get("memory_mb", 0), 2)
            delta_mb = round(result.get("delta_mb", 0), 2)
            delta_sign = "+" if delta_mb > 0 else ""
            description = result.get("description", "")
            content += f"| {name} | {memory_mb} | {delta_sign}{delta_mb} | {description} |\n"
    else:
        content += f"âŒ Erreur mesure mÃ©moire : {memory_results['error']}\n"

    content += "\n## Informations SystÃ¨me\n\n"
    content += f"- **Framework** : QFrame v{perf_results.get('framework_version', 'unknown')}\n"
    content += f"- **Python** : {sys.version.split()[0]}\n"
    content += f"- **Platform** : {sys.platform}\n"

    # Ajouter recommandations performance
    content += "\n## Recommandations Performance\n\n"

    import_time = perf_results.get("benchmarks", {}).get("framework_import", {}).get("time_seconds", 0)
    if import_time > 1.0:
        content += "âš ï¸ **Import Framework** : Temps d'import Ã©levÃ© (> 1s). ConsidÃ©rer lazy loading.\n"
    elif import_time < 0.5:
        content += "âœ… **Import Framework** : Temps d'import optimal (< 0.5s).\n"

    container_time = perf_results.get("benchmarks", {}).get("di_container_creation", {}).get("time_seconds", 0)
    if container_time > 0.1:
        content += "âš ï¸ **Container DI** : CrÃ©ation lente (> 100ms). VÃ©rifier enregistrements.\n"
    elif container_time < 0.05:
        content += "âœ… **Container DI** : CrÃ©ation rapide (< 50ms).\n"

    total_memory = memory_results.get("benchmarks", {}).get("after_container", {}).get("memory_mb", 0)
    if total_memory > 100:
        content += "âš ï¸ **MÃ©moire** : Usage Ã©levÃ© (> 100MB). Optimiser imports.\n"
    elif total_memory < 50:
        content += "âœ… **MÃ©moire** : Usage optimal (< 50MB).\n"

    content += "\n## Graphiques\n\n"
    content += "```mermaid\n"
    content += "graph TD\n"
    content += "    A[Import Framework] --> B[CrÃ©ation Container]\n"
    content += "    B --> C[Chargement Config]\n"
    content += "    C --> D[RÃ©solution StratÃ©gies]\n"
    content += f"    A -.-> A1[{import_time*1000:.1f}ms]\n"
    content += f"    B -.-> B1[{container_time*1000:.1f}ms]\n"
    content += "```\n"

    return content

def generate_coverage_integration() -> str:
    """GÃ©nÃ¨re l'intÃ©gration avec les rapports de coverage."""
    content = """# ğŸ“ˆ Coverage Reports

## Test Coverage

La coverage des tests est automatiquement gÃ©nÃ©rÃ©e et intÃ©grÃ©e dans cette documentation.

```bash
# GÃ©nÃ©rer rapport coverage
poetry run pytest --cov=qframe --cov-report=html --cov-report=xml

# Voir rapport
open htmlcov/index.html
```

## IntÃ©gration Continue

Les mÃ©triques de coverage sont trackÃ©es dans le CI/CD :

- **Target** : > 75%
- **Branches** : > 70%
- **Fonctions** : > 80%

{{ coverage_badge() }}

## Coverage par Module

Les dÃ©tails de coverage par module sont disponibles dans le rapport HTML gÃ©nÃ©rÃ© automatiquement.
"""
    return content

def main():
    """Fonction principale."""
    print("ğŸš€ GÃ©nÃ©ration benchmarks QFrame...")

    # CrÃ©er rÃ©pertoire performance
    docs_dir = project_root / "docs"
    perf_dir = docs_dir / "performance"
    perf_dir.mkdir(exist_ok=True)

    # ExÃ©cuter benchmarks
    print("ğŸ“Š ExÃ©cution benchmarks performance...")
    perf_results = run_performance_tests()

    print("ğŸ’¾ Mesure utilisation mÃ©moire...")
    memory_results = run_memory_benchmarks()

    # GÃ©nÃ©rer rapport
    print("ğŸ“ GÃ©nÃ©ration rapport...")
    report_content = generate_benchmark_report(perf_results, memory_results)

    # Sauvegarder rapport
    with open(perf_dir / "benchmarks.md", "w", encoding="utf-8") as f:
        f.write(report_content)

    # Sauvegarder donnÃ©es JSON pour autres usages
    with open(perf_dir / "benchmarks.json", "w", encoding="utf-8") as f:
        json.dump({
            "performance": perf_results,
            "memory": memory_results
        }, f, indent=2)

    # GÃ©nÃ©rer page coverage
    coverage_content = generate_coverage_integration()
    coverage_dir = docs_dir / "coverage"
    coverage_dir.mkdir(exist_ok=True)

    with open(coverage_dir / "index.md", "w", encoding="utf-8") as f:
        f.write(coverage_content)

    print(f"âœ… Benchmarks gÃ©nÃ©rÃ©s dans {perf_dir}")
    print(f"âœ… Coverage setup dans {coverage_dir}")

if __name__ == "__main__":
    main()